# LP-MusicDialog (or TalkPlayData 0.5): Generating Music Discovery Dialogues

In 2024 Nov, we were honored to present our latest research, "Music Discovery Dialogue Generation Using Human Intent Analysis and Large Language Models," at the 25th International Society for Music Information Retrieval Conference (ISMIR) 2024. This work is a foundational step that enabled our main TalkPlay system, and we sometimes refer to it internally as "TalkPlayData 0.5."

For those interested, the full paper is available on [arXiv](http://arxiv.org/pdf/2411.07439).

## Motivation: Human-like Conversation Dataset Generation

For LLM-based conversational music recommendation system, we need large-scale, high-quality dialogue data. To train a model to help users discover music through natural, multi-turn conversations, you need a vast amount of examples of what those conversations look like. Human-created datasets such as the CPCD dataset are excellent, but often too small to train large models effectively.

Understanding how people naturally discover music through conversation was our starting point. We were curious about the trajectory users take when exploring music through dialogue - what do they ask for, how do they express their preferences, and what conversational patterns emerge during music discovery? To systematically analyze users' information-seeking behavior in music discovery, we examined the existing human-to-human music dialogue dataset (CPCD) using the grounded theory approach. Grounded theory is a qualitative research methodology that develops refined theoretical frameworks from unstructured real-world data, making it ideal for understanding the nuanced patterns in natural music discovery conversations.

## User Intent & System Action in Conversation

![intent_action](https://i.imgur.com/hWENf27.png){: width="100%"}

Table 1 outlines four main categories of user intents with eight sub-intents. These include start-dialogue intents like Initial Query and Greetings, item-discovery-query for expressing music preferences (through Positive/Negative Filters or Continue), Item Attribute Questions about specific track details, and Feedback responses where users Accept or Reject recommendations.

Table 2 outlines the system action with four main categories: requests (for feedback and details), recommendations (both passive/query-based and active/context-based), attribute answers (responding to specific music questions), and general responses (parroting user preferences or showing empathy). These actions enable the system to engage naturally with users while gathering information and providing relevant music suggestions.

## Musical Needs in Conversation

![musical_needs](https://i.imgur.com/4DzXchx.png){: width="100%"}

In Table 3, We categorized musical attributes into four main groups: objective metadata (like track, artist, year, popularity, and culture), subjective similarity (connections between tracks or artists), user context (demographics and listening themes), and music content information (genre, timbre, tempo, etc.) - all of which play crucial roles in understanding and responding to user preferences in music discovery conversations.

## After Analysis, Generate Human Like Conversation Dataset

![generate](https://i.imgur.com/RoHBlWF.png){: width="100%"}

After qualitative analysis, we developed a novel framework to synthetically generate rich music discovery dialogues. Our approach doesn't just have a language model chat with itself; it's a structured process designed to create realistic and useful data. It involves three key stages:

1.  **Dialogue Intent Sequence**: First, we create conversation templates by inheriting the intent sequence patterns we analyzed from human dialogues. This helps ensure our generated conversations follow natural human interaction flows.


2.  **Attribute Sequence Generation**: Next, we generate sequences of music attributes through cascading database filtering, applying multiple filters to progressively narrow down the track selection based on the conversation flow.

3.  **Utterance Generation**: Finally, these structured sequences of user intents and music attributes are fed to a LLM, which translates them into natural, human-like dialogue.

## The Result: LP-MusicDialog

Using this framework, we produced **LP-MusicDialog**, a new synthetic dataset containing over 288,000 music conversations. Our evaluations showed that this dataset is competitive with smaller, human-generated datasets in terms of dialogue consistency and the relevance of the music recommended.

```python
[{'role': 'user',
  'content': "Hi there! I'm in the mood for optimistic music. Can you recommend something uplifting?",
  'intent': ['initial_query', 'add_filter', 'greeting'],
  'music_attribute': ['mood:optimistic']},
 {'role': 'assistant',
  'content': "Sure, could you tell me more about the type of reggae music you enjoy? I'm here to assist you.",
  'intent': ['detail_attribute_request'],
  'music_attribute': [],
  'track_seq': ['SOEZOOE12A8C139302',
   'SOCHTHV12A6D4F9791',
   'SOKAFFC12A6D4FBB10',
    ...]},
 {'role': 'user',
  'content': "I'd like to explore reggae music next.",
  'intent': ['add_filter'],
  'music_attribute': ['genre:reggae']},
 {'role': 'assistant',
  'content': 'Let me find some reggae tracks for you to enjoy.',
  'intent': ['passive_recommend'],
  'music_attribute': [],
  'track_seq': ['SOFDDZV12A8C1388A5',
   'SOIPZNK12A8C1388CB',
   ...]},
 {'role': 'user',
  'content': "You want to continue exploring more music? Let's continue the musical journey together.",
  'intent': ['continue', 'accept_response'],
  'music_attribute': ['continue:continue']},
 {'role': 'assistant',
  'content': "Looking for more music? Let's dive deeper into the world of beats and melodies.",
  'intent': ['passive_recommend', 'parroting_response'],
  'music_attribute': [],
  'track_seq': ['SOZITGZ12A8C1400A9',
   'SOTONHX12A58A7B308',
   'SOQDBIT12A6D4F7DCC',
   ...]},
 {'role': 'user',
  'content': "I'm interested in exploring rap music now.",
  'intent': ['accept_response', 'add_filter'],
  'music_attribute': ['genre:rap']},
 {'role': 'assistant',
  'content': "Let's dive into the world of rap music. Get ready for some rhythmic and powerful beats.",
  'intent': ['passive_recommend'],
  'music_attribute': [],
  'track_seq': ['SODLFHZ12A58A7D312',
   'SOQXUDY12A6D4F906E',
   'SOGNPYN12A6D4F9078',
   ...]},
 {'role': 'user',
  'content': "I'm looking for rap music with a fast tempo.",
  'intent': ['add_filter'],
  'music_attribute': ['tempo:fast tempo']},
 {'role': 'assistant',
  'content': "Got it! I'll search for rap tracks with a fast tempo for you.",
  'intent': ['parroting_response', 'passive_recommend'],
  'music_attribute': [],
  'track_seq': ['SOFZAUC12A81C2179E',
   'SOGOXKR12A81C21799',
   'SOCSGTU12A6D4F9075',
   ...]}]
```

This work was crucial for the [TalkPlay 1 model](https://arxiv.org/abs/2502.13713), as it provided the foundational data needed to train a system that can understand and participate in nuanced music discovery conversations.

## What's Not Done Yet
- The LLM was able to generate conversation, based on the rich text data we provided. But can we make it listen to the audio, directly?
- The outcome, synthetic conversation, is an output of LLM when it is conditioned on a set of tracks and a prompt. The track set varied across each conversation, but not the prompt. This resulted in many conversations sharing some similarity, in terms of how they talk. We would want to cover a wider range in this aspect.
